{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL UNICOS desde XLSX Multi-Hoja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìò Documento de Reglas y Contrato del ETL UNICOS\n",
    "\n",
    "Consulta `ETL_UNICOS_CONTRATO.md` para el detalle del contrato.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Diagn√≥stico (issues detectados en el notebook original)\n",
    "\n",
    "- Widgets usados pero no definidos en el callback (`execute_button`, `manual_path_text`, `max_sheet_input`, `include_aux_checkbox`, `include_otro_checkbox`, `sheet_pattern_text`, `output_status`, `csv_path_text`, `xlsx_path_text`).\n",
    "- Duplicaci√≥n de `export_results()` que genera ambig√ºedad y errores silenciosos.\n",
    "- `filter_rows()` dejaba pasar descripciones con espacios (ej: `\"   \"`).\n",
    "- `read_sheet_fast()` atrapaba errores y retornaba `None` sin trazabilidad.\n",
    "\n",
    "Este notebook corrige todo lo anterior y deja un flujo **Run All safe** con UI completa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SETUP: Instalando dependencias...\n",
      "============================================================\n",
      "Python executable: /home/codevars/Projects/ETL_EXCEL APUS/.venv/bin/python\n",
      "Python version: 3.12.3\n",
      "‚úì pandas ya est√° instalado\n",
      "‚úì numpy ya est√° instalado\n",
      "‚úì openpyxl ya est√° instalado\n",
      "‚úì ipywidgets ya est√° instalado\n",
      "\n",
      "‚è≥ Verificando python-calamine (lectura ultra-rapida)...\n",
      "‚Ñπ python-calamine no disponible (usando openpyxl)\n",
      "  Para instalar: pip install python-calamine\n",
      "\n",
      "‚è≥ Verificando itables (visor interactivo)...\n",
      "‚Ñπ itables no disponible\n",
      "  Para instalar: pip install itables\n",
      "\n",
      "============================================================\n",
      "‚úì Entorno configurado correctamente\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Setup de entorno\n",
    "# Esta celda prepara el entorno del notebook:\n",
    "# 1) Verifica/instala dependencias\n",
    "# 2) Detecta librerias opcionales\n",
    "# 3) Importa todo lo necesario para el ETL\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "import warnings\n",
    "import subprocess\n",
    "import importlib.util\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\"*60)\n",
    "print(\"SETUP: Instalando dependencias...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Paquetes requeridos para el flujo principal\n",
    "required_packages = {\n",
    "    'pandas': 'pandas',\n",
    "    'numpy': 'numpy',\n",
    "    'openpyxl': 'openpyxl',\n",
    "    'ipywidgets': 'ipywidgets',\n",
    "}\n",
    "\n",
    "def install_if_missing(package_name, import_name=None):\n",
    "    \"\"\"Instala el paquete solo si no existe en el kernel actual.\"\"\"\n",
    "    import_name = import_name or package_name\n",
    "    if importlib.util.find_spec(import_name) is not None:\n",
    "        print(f\"‚úì {package_name} ya est√° instalado\")\n",
    "        return\n",
    "\n",
    "    print(f\"‚è≥ Instalando {package_name}...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_name],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True,\n",
    "        )\n",
    "        if result.stdout.strip():\n",
    "            print(result.stdout.strip())\n",
    "        if result.stderr.strip():\n",
    "            print(result.stderr.strip())\n",
    "        print(f\"‚úì {package_name} instalado\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚úó Error instalando {package_name}\")\n",
    "        if e.stdout:\n",
    "            print(e.stdout)\n",
    "        if e.stderr:\n",
    "            print(e.stderr)\n",
    "        raise\n",
    "\n",
    "    if importlib.util.find_spec(import_name) is None:\n",
    "        raise ImportError(f\"No se pudo importar {import_name} despu√©s de la instalaci√≥n\")\n",
    "\n",
    "for pkg, imp in required_packages.items():\n",
    "    install_if_missing(pkg, imp)\n",
    "\n",
    "# Opcional: python-calamine para lectura ultra r√°pida\n",
    "print(\"\")\n",
    "print(\"‚è≥ Verificando python-calamine (lectura ultra-rapida)...\")\n",
    "try:\n",
    "    import calamine\n",
    "    CALAMINE_AVAILABLE = True\n",
    "    print(\"‚úì python-calamine disponible\")\n",
    "except ImportError:\n",
    "    CALAMINE_AVAILABLE = False\n",
    "    print(\"‚Ñπ python-calamine no disponible (usando openpyxl)\")\n",
    "    print(\"  Para instalar: pip install python-calamine\")\n",
    "\n",
    "# Opcional: itables para visualizaci√≥n interactiva\n",
    "print(\"\")\n",
    "print(\"‚è≥ Verificando itables (visor interactivo)...\")\n",
    "try:\n",
    "    import itables\n",
    "    ITABLES_AVAILABLE = True\n",
    "    print(\"‚úì itables disponible\")\n",
    "except ImportError:\n",
    "    ITABLES_AVAILABLE = False\n",
    "    print(\"‚Ñπ itables no disponible\")\n",
    "    print(\"  Para instalar: pip install itables\")\n",
    "\n",
    "# Importar dependencias luego de instalarlas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import FileUpload, VBox, HBox, Button, Label, HTML, Output, IntText, Text, Checkbox, Dropdown\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úì Entorno configurado correctamente\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìÅ CONFIGURACI√ìN DE CARPETAS\n",
      "======================================================================\n",
      "Carpeta de entrada: /home/codevars/Projects/ETL_EXCEL APUS/input\n",
      "Carpeta de salida: /home/codevars/Projects/ETL_EXCEL APUS/output\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Carpeta de entrada configurada correctamente\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Configuraci√≥n de carpeta de entrada y carga de archivos\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Crear carpeta de entrada si no existe\n",
    "INPUT_FOLDER = Path('./input')\n",
    "INPUT_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "OUTPUT_FOLDER = Path('./output')\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "# Diccionario global para almacenar la ruta actual del archivo\n",
    "current_file_path = {'path': None}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìÅ CONFIGURACI√ìN DE CARPETAS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Carpeta de entrada: {INPUT_FOLDER.absolute()}\")\n",
    "print(f\"Carpeta de salida: {OUTPUT_FOLDER.absolute()}\")\n",
    "\n",
    "# UI para cargar archivo desde carpeta de entrada\n",
    "input_status = Output()\n",
    "\n",
    "input_file_upload = FileUpload(\n",
    "    accept='.xlsx',\n",
    "    multiple=False,\n",
    "    description='üì§ Selecciona tu Excel:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "def list_input_files():\n",
    "    \"\"\"Lista archivos Excel en la carpeta de entrada.\"\"\"\n",
    "    excel_files = list(INPUT_FOLDER.glob('*.xlsx')) + list(INPUT_FOLDER.glob('*.xls'))\n",
    "    return excel_files\n",
    "\n",
    "def handle_input_upload(change):\n",
    "    \"\"\"Guarda el archivo en la carpeta de entrada.\"\"\"\n",
    "    input_status.clear_output()\n",
    "    if input_file_upload.value:\n",
    "        uploaded_files = input_file_upload.value\n",
    "        if isinstance(uploaded_files, tuple):\n",
    "            for file_info in uploaded_files:\n",
    "                filename = file_info.get('name', 'archivo.xlsx')\n",
    "                content = file_info.get('content', b'')\n",
    "                input_path = INPUT_FOLDER / filename\n",
    "                with open(input_path, 'wb') as f:\n",
    "                    f.write(content)\n",
    "                current_file_path['path'] = str(input_path)\n",
    "                with input_status:\n",
    "                    print(f\"‚úÖ Archivo cargado: {filename}\")\n",
    "                    print(f\"üìç Ubicaci√≥n: {input_path.absolute()}\")\n",
    "        else:\n",
    "            for filename, filedata in uploaded_files.items():\n",
    "                input_path = INPUT_FOLDER / filename\n",
    "                with open(input_path, 'wb') as f:\n",
    "                    f.write(filedata['content'])\n",
    "                current_file_path['path'] = str(input_path)\n",
    "                with input_status:\n",
    "                    print(f\"‚úÖ Archivo cargado: {filename}\")\n",
    "                    print(f\"üìç Ubicaci√≥n: {input_path.absolute()}\")\n",
    "\n",
    "input_file_upload.observe(handle_input_upload, names='value')\n",
    "\n",
    "# Bot√≥n para limpiar carpeta\n",
    "clean_button = Button(\n",
    "    description='üóëÔ∏è Limpiar entrada',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "def on_clean_click(b):\n",
    "    \"\"\"Limpia la carpeta de entrada.\"\"\"\n",
    "    with input_status:\n",
    "        input_status.clear_output()\n",
    "        try:\n",
    "            if list(INPUT_FOLDER.glob('*')):\n",
    "                shutil.rmtree(INPUT_FOLDER)\n",
    "                INPUT_FOLDER.mkdir(exist_ok=True)\n",
    "                current_file_path['path'] = None\n",
    "                print(\"‚úÖ Carpeta de entrada limpiada\")\n",
    "            else:\n",
    "                print(\"‚ÑπÔ∏è La carpeta ya est√° vac√≠a\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al limpiar: {e}\")\n",
    "\n",
    "clean_button.on_click(on_clean_click)\n",
    "\n",
    "# Bot√≥n para listar archivos\n",
    "list_button = Button(\n",
    "    description='üìã Listar archivos',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "def on_list_click(b):\n",
    "    \"\"\"Lista los archivos en la carpeta de entrada.\"\"\"\n",
    "    with input_status:\n",
    "        input_status.clear_output()\n",
    "        files = list_input_files()\n",
    "        if files:\n",
    "            print(\"üìÇ Archivos en la carpeta de entrada:\")\n",
    "            for i, f in enumerate(files, 1):\n",
    "                size_mb = f.stat().st_size / (1024*1024)\n",
    "                selected = \" ‚Üê SELECCIONADO\" if current_file_path['path'] == str(f) else \"\"\n",
    "                print(f\"  {i}. {f.name} ({size_mb:.2f} MB){selected}\")\n",
    "        else:\n",
    "            print(\"üì≠ No hay archivos Excel en la carpeta de entrada\")\n",
    "\n",
    "list_button.on_click(on_list_click)\n",
    "\n",
    "# Construir panel de carga (se mostrar√° en el Panel ETL)\n",
    "input_ui = VBox([\n",
    "    HTML('<h3>üì• Carga de Archivo</h3>'),\n",
    "    input_file_upload,\n",
    "    HBox([clean_button, list_button]),\n",
    "    input_status\n",
    "])\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Carpeta de entrada configurada correctamente\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuraci√≥n global cargada\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Configuraci√≥n global\n",
    "# Mapeo de columnas del Excel hacia nombres internos\n",
    "# Nota: solo se leen columnas espec√≠ficas para optimizar rendimiento.\n",
    "COLUMN_MAPPING = {\n",
    "    'A': 'Column1',\n",
    "    'B': 'Column2',\n",
    "    'C': 'Column3',\n",
    "    'E': 'Column5',\n",
    "    'G': 'Column7',\n",
    "    'J': 'Column10',\n",
    "    'K': 'Column11',\n",
    "    'L': 'Column12',\n",
    "}\n",
    "\n",
    "# Clasificaci√≥n por primera letra\n",
    "RESOURCE_TYPE_MAP = {\n",
    "    'M': 'EQUIPO',\n",
    "    'N': 'MANO DE OBRA',\n",
    "    'O': 'MATERIAL',\n",
    "    'P': 'TRANSPORTE',\n",
    "}\n",
    "\n",
    "# Regex para detectar c√≥digos embebidos (ej: M.01, N02, O-03)\n",
    "CODE_PATTERN = r'([MNOP])\\s*[\\.-]?\\s*\\d+'\n",
    "\n",
    "# Patrones de encabezados que deben excluirse\n",
    "SECTION_HEADERS_PATTERNS = [\n",
    "    r'^EQUIPOS',\n",
    "    r'^MANO\\s+DE\\s+OBRA',\n",
    "    r'^MATERIALES?',\n",
    "    r'^TRANSPORTE',\n",
    "]\n",
    "\n",
    "def is_section_header(text: str) -> bool:\n",
    "    \"\"\"Detecta si un texto es un encabezado de secci√≥n.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return False\n",
    "    text_upper = str(text).strip().upper()\n",
    "    for pattern in SECTION_HEADERS_PATTERNS:\n",
    "        if re.match(pattern, text_upper):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"‚úì Configuraci√≥n global cargada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Funciones ETL - Selecci√≥n de hojas y lectura\n",
    "def pick_sheets(\n",
    "    xlsx_path: str,\n",
    "    sheet_pattern: str = r'^\\d{3}$',\n",
    "    max_sheet: int = None,\n",
    "    include_aux: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Selecciona hojas seg√∫n patr√≥n regex y l√≠mite.\n",
    "    Respeta el orden original del archivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        excel_file = pd.ExcelFile(xlsx_path)\n",
    "        all_sheets = excel_file.sheet_names\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error abriendo {xlsx_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Compilar regex del patr√≥n de hojas\n",
    "    try:\n",
    "        pattern = re.compile(sheet_pattern)\n",
    "    except Exception:\n",
    "        print(f\"‚ùå Patr√≥n regex inv√°lido: {sheet_pattern}\")\n",
    "        return []\n",
    "\n",
    "    # Hojas num√©ricas (seg√∫n patr√≥n)\n",
    "    numeric_sheets = [s for s in all_sheets if pattern.match(s)]\n",
    "\n",
    "    # Hojas auxiliares\n",
    "    aux_sheets = [s for s in all_sheets if s.startswith('Aux.')] if include_aux else []\n",
    "\n",
    "    # Combinar respetando orden original del archivo\n",
    "    selected = [s for s in all_sheets if s in numeric_sheets or s in aux_sheets]\n",
    "\n",
    "    # Limitar cantidad de hojas si se pide\n",
    "    if max_sheet and max_sheet > 0:\n",
    "        selected = selected[:max_sheet]\n",
    "\n",
    "    return selected\n",
    "\n",
    "def read_sheet_fast(\n",
    "    xlsx_path: str,\n",
    "    sheet_name: str,\n",
    "    column_mapping: Dict[str, str] = COLUMN_MAPPING,\n",
    "    return_error: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Lee solo columnas necesarias de una hoja.\n",
    "    Intenta calamine primero (r√°pido), fallback a openpyxl.\n",
    "    Si return_error=True, retorna (df, error).\n",
    "    \"\"\"\n",
    "    calamine_error = None\n",
    "\n",
    "    # Intentar con python-calamine si est√° disponible\n",
    "    if CALAMINE_AVAILABLE:\n",
    "        try:\n",
    "            from calamine import load_workbook\n",
    "            wb = load_workbook(xlsx_path)\n",
    "            ws = wb.sheet_by_name(sheet_name)\n",
    "\n",
    "            # Indices 0-based para columnas A, B, C, E, G, J, K, L\n",
    "            col_indices = {'A': 0, 'B': 1, 'C': 2, 'E': 4, 'G': 6, 'J': 9, 'K': 10, 'L': 11}\n",
    "            data = {}\n",
    "\n",
    "            for col_letter, col_name in column_mapping.items():\n",
    "                col_idx = col_indices[col_letter]\n",
    "                col_data = []\n",
    "                for row in ws.rows():\n",
    "                    if col_idx < len(row):\n",
    "                        cell = row[col_idx]\n",
    "                        col_data.append(cell.value if hasattr(cell, 'value') else cell)\n",
    "                    else:\n",
    "                        col_data.append(None)\n",
    "                data[col_name] = col_data\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            return (df, None) if return_error else df\n",
    "        except Exception as e:\n",
    "            calamine_error = f\"calamine: {type(e).__name__}: {e}\"\n",
    "\n",
    "    # Fallback: openpyxl\n",
    "    try:\n",
    "        from openpyxl import load_workbook\n",
    "        wb = load_workbook(xlsx_path, data_only=True, read_only=True)\n",
    "        ws = wb[sheet_name]\n",
    "\n",
    "        # Construir columnas vac√≠as\n",
    "        col_letters = list(column_mapping.keys())\n",
    "        data = {column_mapping[col]: [] for col in col_letters}\n",
    "\n",
    "        # Recorrer filas y extraer columnas clave\n",
    "        for row in ws.iter_rows(min_col=1, max_col=12, values_only=False):\n",
    "            for col_letter, col_name in column_mapping.items():\n",
    "                col_idx = ord(col_letter) - ord('A')\n",
    "                if col_idx < len(row):\n",
    "                    cell = row[col_idx]\n",
    "                    value = cell.value if hasattr(cell, 'value') else cell\n",
    "                    data[col_name].append(value)\n",
    "                else:\n",
    "                    data[col_name].append(None)\n",
    "\n",
    "        wb.close()\n",
    "        df = pd.DataFrame(data)\n",
    "        return (df, None) if return_error else df\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"openpyxl: {type(e).__name__}: {e}\"\n",
    "        if calamine_error:\n",
    "            err = f\"{calamine_error} | {err}\"\n",
    "        return (None, err) if return_error else None\n",
    "\n",
    "print(\"‚úì pick_sheets() y read_sheet_fast() cargados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Funciones de filtrado cargadas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Funciones ETL - Filtrado y clasificaci√≥n\n",
    "def filter_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Excluye:\n",
    "    - Filas con descripci√≥n vac√≠a (Column3)\n",
    "    - Encabezados de secci√≥n\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Filtrar nulos/vac√≠os/espacios en descripci√≥n\n",
    "    desc = df['Column3']\n",
    "    desc_str = desc.astype(str).str.strip()\n",
    "    df = df[desc.notna() & desc_str.ne('')]\n",
    "\n",
    "    # Excluir encabezados de secci√≥n (por Column1 o Column3)\n",
    "    df = df[~df['Column1'].astype(str).apply(is_section_header)]\n",
    "    df = df[~df['Column3'].astype(str).apply(is_section_header)]\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def extract_code(row_dict: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Intenta extraer el c√≥digo desde Column1/Column2/Column3.\n",
    "    Si no encuentra patr√≥n, devuelve texto normalizado del campo m√°s confiable.\n",
    "    \"\"\"\n",
    "    candidates = [row_dict.get('Column1', ''), row_dict.get('Column2', ''), row_dict.get('Column3', '')]\n",
    "    for val in candidates:\n",
    "        if pd.isna(val) or val == '':\n",
    "            continue\n",
    "        m = re.search(CODE_PATTERN, str(val).upper())\n",
    "        if m:\n",
    "            return m.group(0).replace(' ', '')\n",
    "\n",
    "    # Fallback: usar Column1/Column2 sin patr√≥n\n",
    "    fallback = normalize_text_field(row_dict.get('Column1', '')) or normalize_text_field(row_dict.get('Column2', ''))\n",
    "    return fallback\n",
    "\n",
    "def classify_resource(code_or_text: str) -> str:\n",
    "    \"\"\"Clasifica recurso por primera letra encontrada.\"\"\"\n",
    "    text = str(code_or_text).upper() if code_or_text else ''\n",
    "    if text:\n",
    "        letter = text[0]\n",
    "        return RESOURCE_TYPE_MAP.get(letter, 'OTRO')\n",
    "    return 'OTRO'\n",
    "\n",
    "def normalize_numeric(value, is_percentage=False) -> float:\n",
    "    \"\"\"\n",
    "    Normaliza valores num√©ricos con m√∫ltiples formatos:\n",
    "    - \"1,234.56\", \"1.234,56\", \"1234,56\", \"1234.56\", \"10%\", \"0.1\"\n",
    "\n",
    "    Nota: si is_percentage=True, convierte 40 -> 0.40\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return 0.0\n",
    "\n",
    "    value_str = str(value).strip()\n",
    "\n",
    "    # Quitar s√≠mbolo % si existe\n",
    "    if '%' in value_str:\n",
    "        value_str = value_str.replace('%', '').strip()\n",
    "\n",
    "    comma_count = value_str.count(',')\n",
    "    dot_count = value_str.count('.')\n",
    "\n",
    "    try:\n",
    "        if comma_count > 1 or dot_count > 1:\n",
    "            if ',' in value_str and '.' in value_str:\n",
    "                if value_str.rindex(',') > value_str.rindex('.'):\n",
    "                    value_str = value_str.replace('.', '').replace(',', '.')\n",
    "                else:\n",
    "                    value_str = value_str.replace(',', '')\n",
    "            elif comma_count > 1:\n",
    "                value_str = value_str.replace(',', '.', comma_count - 1).replace(',', '.')\n",
    "        elif comma_count == 1 and dot_count == 0:\n",
    "            parts = value_str.split(',')\n",
    "            if len(parts[1]) > 2:\n",
    "                value_str = value_str.replace(',', '')\n",
    "            else:\n",
    "                value_str = value_str.replace(',', '.')\n",
    "\n",
    "        result = float(value_str)\n",
    "\n",
    "        if is_percentage:\n",
    "            result = result / 100.0\n",
    "\n",
    "        return result\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def normalize_text_field(value) -> str:\n",
    "    \"\"\"Normaliza campos de texto.\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return ''\n",
    "    return str(value).strip()\n",
    "\n",
    "print(\"‚úì Funciones de filtrado cargadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì process_row() y build_unicos() cargados\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Funciones ETL - Procesamiento de filas y UNICOS\n",
    "def process_row(row_dict: Dict, debug: bool = False) -> Optional[Dict]:\n",
    "    \"\"\"Procesa una fila y devuelve dict normalizado o None si no es v√°lida.\"\"\"\n",
    "    try:\n",
    "        col1 = row_dict.get('Column1', '')\n",
    "        col2 = row_dict.get('Column2', '')\n",
    "        col3 = row_dict.get('Column3', '')\n",
    "        col5 = row_dict.get('Column5', '')\n",
    "        col7 = row_dict.get('Column7', '')\n",
    "        col10 = row_dict.get('Column10', '')\n",
    "        col11 = row_dict.get('Column11', '')\n",
    "        col12 = row_dict.get('Column12', '')\n",
    "\n",
    "        # Validaci√≥n: debe haber descripci√≥n\n",
    "        descripcion = normalize_text_field(col3)\n",
    "        if descripcion == '':\n",
    "            return None\n",
    "\n",
    "        # Excluir encabezados\n",
    "        if is_section_header(col1) or is_section_header(col3):\n",
    "            return None\n",
    "\n",
    "        # Detectar c√≥digo y clasificar recurso\n",
    "        codigo = extract_code(row_dict)\n",
    "        recurso = classify_resource(codigo)\n",
    "\n",
    "        # PRECIO UNITARIO y UNIDAD seg√∫n RECURSO\n",
    "        if recurso in ('EQUIPO', 'MANO DE OBRA'):\n",
    "            precio_unitario = normalize_numeric(col5)\n",
    "            unidad = ''\n",
    "        else:\n",
    "            precio_unitario = normalize_numeric(col7)\n",
    "            unidad = normalize_text_field(col5)\n",
    "\n",
    "        # Otros campos\n",
    "        cpc = normalize_text_field(col10)\n",
    "        np_nd = normalize_text_field(col11)\n",
    "        vae = normalize_numeric(col12, is_percentage=False)\n",
    "\n",
    "        result = {\n",
    "            'DESCRIPCION': descripcion,\n",
    "            'UNIDAD': unidad,\n",
    "            'PRECIO UNITARIO': precio_unitario,\n",
    "            'CPC ELEMENTO': cpc,\n",
    "            'NP/ND/EP': np_nd if np_nd else '',\n",
    "            'VAE (%)': vae,\n",
    "            'RECURSO': recurso\n",
    "        }\n",
    "\n",
    "        if debug:\n",
    "            result['CODIGO'] = codigo\n",
    "\n",
    "        return result\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_unicos(\n",
    "    all_rows: List[Dict],\n",
    "    include_otro: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construye DataFrame UNICOS final:\n",
    "    - Concatena todas las filas\n",
    "    - Deduplicaci√≥n\n",
    "    - Normalizaci√≥n\n",
    "    - Ordenamiento\n",
    "    \"\"\"\n",
    "    if not all_rows:\n",
    "        print(\"‚ùå No hay filas para procesar\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # Excluir OTRO si aplica\n",
    "    if not include_otro:\n",
    "        df = df[df['RECURSO'] != 'OTRO']\n",
    "\n",
    "    # Columnas finales (sin CODIGO ni HOJA)\n",
    "    final_cols = ['DESCRIPCION', 'UNIDAD', 'PRECIO UNITARIO', 'CPC ELEMENTO', 'NP/ND/EP', 'VAE (%)', 'RECURSO']\n",
    "    df = df[final_cols]\n",
    "\n",
    "    # Deduplicaci√≥n exacta de filas\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Normalizaci√≥n post-dedup\n",
    "    df['NP/ND/EP'] = df['NP/ND/EP'].fillna('').astype(str)\n",
    "    df['VAE (%)'] = df['VAE (%)'].fillna(0).astype(float)\n",
    "    df['PRECIO UNITARIO'] = df['PRECIO UNITARIO'].fillna(0).astype(float)\n",
    "    df['UNIDAD'] = df['UNIDAD'].fillna('').astype(str)\n",
    "\n",
    "    # Ordenar por DESCRIPCION\n",
    "    df = df.sort_values('DESCRIPCION', ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"‚úì process_row() y build_unicos() cargados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì run_etl() cargado\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Ejecuci√≥n del ETL\n",
    "def run_etl(source: str, options: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline ETL completo.\n",
    "\n",
    "    Args:\n",
    "        source: ruta del archivo Excel\n",
    "        options: dict con sheet_pattern, max_sheet, include_aux, include_otro, debug\n",
    "\n",
    "    Returns:\n",
    "        results dict con:\n",
    "        - df_unicos\n",
    "        - df_sheet_summary\n",
    "        - df_debug (si debug)\n",
    "        - df_debug_agg (si debug)\n",
    "        - stats\n",
    "    \"\"\"\n",
    "    options = options or {}\n",
    "    sheet_pattern = options.get('sheet_pattern', r'^\\d{3}$')\n",
    "    max_sheet = options.get('max_sheet') or None\n",
    "    include_aux = options.get('include_aux', True)\n",
    "    include_otro = options.get('include_otro', False)\n",
    "    debug = options.get('debug', False)\n",
    "    progress_every = options.get('progress_every', 20)\n",
    "\n",
    "    stats = {\n",
    "        'total_sheets': 0,\n",
    "        'sheets_ok': 0,\n",
    "        'sheets_failed': 0,\n",
    "        'total_rows_read': 0,\n",
    "        'total_rows_filtered': 0,\n",
    "        'total_rows_extracted': 0,\n",
    "        'duplicates_removed': 0,\n",
    "        'final_rows': 0,\n",
    "        'time_seconds': 0,\n",
    "        'failed_sheet_details': []\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Seleccionar hojas\n",
    "    sheets_to_process = pick_sheets(\n",
    "        source,\n",
    "        sheet_pattern=sheet_pattern,\n",
    "        max_sheet=max_sheet,\n",
    "        include_aux=include_aux\n",
    "    )\n",
    "\n",
    "    stats['total_sheets'] = len(sheets_to_process)\n",
    "    total_sheets = stats['total_sheets']\n",
    "\n",
    "    all_rows = []\n",
    "    debug_rows = []\n",
    "    sheet_summary_rows = []\n",
    "\n",
    "    # 2. Procesar cada hoja\n",
    "    for i, sheet_name in enumerate(sheets_to_process, 1):\n",
    "        if progress_every and (i == 1 or i % progress_every == 0 or i == total_sheets):\n",
    "            print(f\"‚è≥ Progreso: {i}/{total_sheets} hojas... (actual: {sheet_name})\")\n",
    "\n",
    "        df, err = read_sheet_fast(source, sheet_name, return_error=True)\n",
    "\n",
    "        if err or df is None or df.empty:\n",
    "            stats['sheets_failed'] += 1\n",
    "            error_msg = err or 'Hoja vac√≠a o no legible'\n",
    "            stats['failed_sheet_details'].append({\n",
    "                'HOJA': sheet_name,\n",
    "                'ERROR': error_msg\n",
    "            })\n",
    "            sheet_summary_rows.append({\n",
    "                'HOJA': sheet_name,\n",
    "                'ROWS_READ': 0,\n",
    "                'ROWS_FILTERED': 0,\n",
    "                'ROWS_EXTRACTED': 0,\n",
    "                'ROW_ERRORS': 0,\n",
    "                'STATUS': 'FAILED',\n",
    "                'ERROR': error_msg\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Agregar √≠ndice de fila para trazabilidad\n",
    "        df = df.copy()\n",
    "        df['ROW_INDEX'] = df.index + 1\n",
    "\n",
    "        rows_read = len(df)\n",
    "        df_filtered = filter_rows(df)\n",
    "        rows_filtered = len(df_filtered)\n",
    "\n",
    "        # Acumular stats incluso si la hoja queda vac√≠a tras filtrado\n",
    "        stats['total_rows_read'] += rows_read\n",
    "        stats['total_rows_filtered'] += rows_filtered\n",
    "\n",
    "        if df_filtered.empty:\n",
    "            stats['sheets_failed'] += 1\n",
    "            error_msg = 'Sin filas v√°lidas tras filtrado'\n",
    "            stats['failed_sheet_details'].append({\n",
    "                'HOJA': sheet_name,\n",
    "                'ERROR': error_msg\n",
    "            })\n",
    "            sheet_summary_rows.append({\n",
    "                'HOJA': sheet_name,\n",
    "                'ROWS_READ': rows_read,\n",
    "                'ROWS_FILTERED': 0,\n",
    "                'ROWS_EXTRACTED': 0,\n",
    "                'ROW_ERRORS': 0,\n",
    "                'STATUS': 'FAILED',\n",
    "                'ERROR': error_msg\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        rows_extracted = 0\n",
    "        row_errors = 0\n",
    "\n",
    "        for _, row in df_filtered.iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            processed = process_row(row_dict, debug=debug)\n",
    "            if processed:\n",
    "                rows_extracted += 1\n",
    "                if debug:\n",
    "                    processed['HOJA'] = sheet_name\n",
    "                    processed['ROW_INDEX'] = int(row_dict.get('ROW_INDEX', 0))\n",
    "                    debug_rows.append(processed.copy())\n",
    "                all_rows.append(processed)\n",
    "            else:\n",
    "                row_errors += 1\n",
    "\n",
    "        stats['sheets_ok'] += 1\n",
    "        stats['total_rows_extracted'] += rows_extracted\n",
    "\n",
    "        sheet_summary_rows.append({\n",
    "            'HOJA': sheet_name,\n",
    "            'ROWS_READ': rows_read,\n",
    "            'ROWS_FILTERED': rows_filtered,\n",
    "            'ROWS_EXTRACTED': rows_extracted,\n",
    "            'ROW_ERRORS': row_errors,\n",
    "            'STATUS': 'OK' if row_errors == 0 else 'OK_WITH_ERRORS',\n",
    "            'ERROR': ''\n",
    "        })\n",
    "\n",
    "    # 3. Construir UNICOS\n",
    "    df_unicos = build_unicos(all_rows, include_otro=include_otro)\n",
    "\n",
    "    stats['final_rows'] = len(df_unicos)\n",
    "    stats['duplicates_removed'] = stats['total_rows_extracted'] - stats['final_rows']\n",
    "    stats['time_seconds'] = time.time() - start_time\n",
    "\n",
    "    # 4. Sheet summary\n",
    "    if sheet_summary_rows:\n",
    "        df_sheet_summary = pd.DataFrame(sheet_summary_rows)\n",
    "        df_sheet_summary = df_sheet_summary.sort_values('HOJA', ignore_index=True)\n",
    "    else:\n",
    "        df_sheet_summary = pd.DataFrame(columns=[\n",
    "            'HOJA', 'ROWS_READ', 'ROWS_FILTERED', 'ROWS_EXTRACTED', 'ROW_ERRORS', 'STATUS', 'ERROR'\n",
    "        ])\n",
    "\n",
    "    # 5. Debug outputs\n",
    "    df_debug = None\n",
    "    df_debug_agg = None\n",
    "    if debug and debug_rows:\n",
    "        df_debug = pd.DataFrame(debug_rows)\n",
    "\n",
    "        final_cols = ['DESCRIPCION', 'UNIDAD', 'PRECIO UNITARIO', 'CPC ELEMENTO', 'NP/ND/EP', 'VAE (%)', 'RECURSO']\n",
    "\n",
    "        def unique_list(series):\n",
    "            seen = set()\n",
    "            out = []\n",
    "            for v in series:\n",
    "                if pd.isna(v):\n",
    "                    continue\n",
    "                v_str = str(v).strip()\n",
    "                if v_str == '':\n",
    "                    continue\n",
    "                if v_str not in seen:\n",
    "                    seen.add(v_str)\n",
    "                    out.append(v_str)\n",
    "            return out\n",
    "\n",
    "        df_debug_agg = (\n",
    "            df_debug\n",
    "            .groupby(final_cols, dropna=False)\n",
    "            .agg(\n",
    "                HOJAS=('HOJA', unique_list),\n",
    "                CODIGOS=('CODIGO', unique_list),\n",
    "                FILAS=('HOJA', 'count')\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "    results = {\n",
    "        'df_unicos': df_unicos,\n",
    "        'df_sheet_summary': df_sheet_summary,\n",
    "        'df_debug': df_debug,\n",
    "        'df_debug_agg': df_debug_agg,\n",
    "        'stats': stats\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"‚úì run_etl() cargado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# %% Similitud heur√≠stica (funciones)\n",
    "# Configuraci√≥n\n",
    "SIM_THRESHOLD = 90\n",
    "SIM_SCOPE = 'RECURSO'\n",
    "SIM_EXPORT_PATH = 'output/unicos_similitud.xlsx'\n",
    "SIM_PROGRESS_EVERY = 100\n",
    "\n",
    "# Motor de similitud\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "    RAPIDFUZZ_AVAILABLE = True\n",
    "except Exception:\n",
    "    RAPIDFUZZ_AVAILABLE = False\n",
    "\n",
    "import difflib\n",
    "\n",
    "def normalize_desc(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    s = str(text).lower().strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_similarity(a: str, b: str) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    if RAPIDFUZZ_AVAILABLE:\n",
    "        return float(fuzz.ratio(a, b))\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio() * 100.0\n",
    "\n",
    "\n",
    "def find_similar_candidates(\n",
    "    df_unicos: pd.DataFrame,\n",
    "    threshold: int = SIM_THRESHOLD,\n",
    "    scope: str = SIM_SCOPE,\n",
    "    progress_every: int = SIM_PROGRESS_EVERY\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encuentra pares de descripciones con similitud >= threshold.\n",
    "    Scope por defecto: mismo RECURSO.\n",
    "    \"\"\"\n",
    "    if df_unicos is None or df_unicos.empty:\n",
    "        return pd.DataFrame(columns=['DESCRIPCION_A', 'DESCRIPCION_B', 'SIMILARIDAD', 'RECURSO'])\n",
    "\n",
    "    df = df_unicos.copy()\n",
    "    df['DESC_NORM'] = df['DESCRIPCION'].apply(normalize_desc)\n",
    "    df = df[df['DESC_NORM'] != '']\n",
    "\n",
    "    records = []\n",
    "    groups = df.groupby('RECURSO') if scope == 'RECURSO' else [('ALL', df)]\n",
    "\n",
    "    for recurso, g in groups:\n",
    "        g = g[['DESCRIPCION', 'DESC_NORM']].drop_duplicates().copy()\n",
    "        if g.empty:\n",
    "            continue\n",
    "\n",
    "        g['BLOCK'] = g['DESC_NORM'].str[:4]\n",
    "        blocks = g.groupby('BLOCK')\n",
    "\n",
    "        comparisons = 0\n",
    "        for block_key, gb in blocks:\n",
    "            items = gb.to_dict('records')\n",
    "            n = len(items)\n",
    "            if n < 2:\n",
    "                continue\n",
    "            for i in range(n):\n",
    "                a = items[i]\n",
    "                for j in range(i + 1, n):\n",
    "                    b = items[j]\n",
    "                    sim = get_similarity(a['DESC_NORM'], b['DESC_NORM'])\n",
    "                    comparisons += 1\n",
    "                    if progress_every and comparisons % progress_every == 0:\n",
    "                        print(f\"‚è≥ Similitud: {comparisons} comparaciones (RECURSO={recurso})\")\n",
    "                    if sim >= threshold:\n",
    "                        records.append({\n",
    "                            'DESCRIPCION_A': a['DESCRIPCION'],\n",
    "                            'DESCRIPCION_B': b['DESCRIPCION'],\n",
    "                            'SIMILARIDAD': round(sim, 2),\n",
    "                            'RECURSO': recurso\n",
    "                        })\n",
    "\n",
    "    if not records:\n",
    "        return pd.DataFrame(columns=['DESCRIPCION_A', 'DESCRIPCION_B', 'SIMILARIDAD', 'RECURSO'])\n",
    "\n",
    "    df_candidates = pd.DataFrame(records)\n",
    "    df_candidates = df_candidates.sort_values('SIMILARIDAD', ascending=False)\n",
    "\n",
    "    # Mantener mejor match por DESCRIPCION_A\n",
    "    df_candidates = df_candidates.drop_duplicates(subset=['DESCRIPCION_A'], keep='first')\n",
    "\n",
    "    return df_candidates.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def build_canonical_map(df_unicos: pd.DataFrame, df_candidates: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Propone un nombre can√≥nico por cada candidato.\n",
    "    Regla: m√°s frecuente, desempate por menor longitud.\n",
    "    \"\"\"\n",
    "    if df_candidates is None or df_candidates.empty:\n",
    "        return pd.DataFrame(columns=['ORIGINAL', 'SIMILAR', 'PROPUESTA', 'SIMILARIDAD', 'RECURSO'])\n",
    "\n",
    "    freq = df_unicos['DESCRIPCION'].value_counts().to_dict()\n",
    "\n",
    "    suggestions = []\n",
    "    for _, row in df_candidates.iterrows():\n",
    "        a = row['DESCRIPCION_A']\n",
    "        b = row['DESCRIPCION_B']\n",
    "        fa = freq.get(a, 0)\n",
    "        fb = freq.get(b, 0)\n",
    "\n",
    "        if fa > fb:\n",
    "            propuesta = a\n",
    "        elif fb > fa:\n",
    "            propuesta = b\n",
    "        else:\n",
    "            propuesta = a if len(a) <= len(b) else b\n",
    "\n",
    "        suggestions.append({\n",
    "            'ORIGINAL': a,\n",
    "            'SIMILAR': b,\n",
    "            'PROPUESTA': propuesta,\n",
    "            'SIMILARIDAD': row['SIMILARIDAD'],\n",
    "            'RECURSO': row['RECURSO']\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(suggestions)\n",
    "\n",
    "\n",
    "def export_similarity_results(df_unicos_revisado: pd.DataFrame, df_mapeo: pd.DataFrame, path: str = SIM_EXPORT_PATH) -> Tuple[bool, str]:\n",
    "    if df_unicos_revisado is None or df_unicos_revisado.empty:\n",
    "        return False, \"‚ö†Ô∏è df_unicos_revisado est√° vac√≠o\"\n",
    "    if df_mapeo is None or df_mapeo.empty:\n",
    "        return False, \"‚ö†Ô∏è df_mapeo est√° vac√≠o\"\n",
    "\n",
    "    out_path = Path(path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(out_path, engine='openpyxl') as writer:\n",
    "            df_unicos_revisado.to_excel(writer, sheet_name='UNICOS', index=False)\n",
    "            df_mapeo.to_excel(writer, sheet_name='MAPEO', index=False)\n",
    "        return True, f\"‚úÖ XLSX similitud: {out_path.absolute()}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"‚ùå Error exportando similitud: {e}\"\n",
    "\n",
    "print(\"‚úì Funciones de similitud cargadas\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì export_results() disponible\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Exportaci√≥n de resultados (√∫nica definici√≥n)\n",
    "def export_results(df: pd.DataFrame, csv_path: str, xlsx_path: str, sheet_name: str = 'UNICOS') -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Exporta resultados a CSV y XLSX.\n",
    "\n",
    "    Returns:\n",
    "        (ok, message)\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return False, \"‚ö†Ô∏è No hay resultados para exportar\"\n",
    "\n",
    "    if not csv_path or not xlsx_path:\n",
    "        return False, \"‚ö†Ô∏è Rutas de exportaci√≥n inv√°lidas\"\n",
    "\n",
    "    csv_path = Path(csv_path)\n",
    "    xlsx_path = Path(xlsx_path)\n",
    "\n",
    "    # Asegurar que existan las carpetas\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    xlsx_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Exportar CSV\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        # Exportar XLSX\n",
    "        with pd.ExcelWriter(xlsx_path, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        msg = f\"\"\"‚úÖ CSV: {csv_path.absolute()}\n",
    "‚úÖ XLSX: {xlsx_path.absolute()}\n",
    "\n",
    "üì¶ Archivos guardados en carpeta: {OUTPUT_FOLDER.absolute()}\"\"\"\n",
    "        return True, msg\n",
    "    except Exception as e:\n",
    "        return False, f\"‚ùå Error en exportaci√≥n: {e}\"\n",
    "\n",
    "print(\"‚úì export_results() disponible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f013c83ed6504a789bc3b5babecf5b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>‚öôÔ∏è Panel de Control ETL</h2>'), VBox(children=(HTML(value='<h3>üì• Carga de Archi‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %% Panel ETL (UI y callbacks)\n",
    "state = {\n",
    "    'df_unicos': None,\n",
    "    'df_sheet_summary': None,\n",
    "    'df_debug': None,\n",
    "    'df_debug_agg': None,\n",
    "    'stats': None,\n",
    "    'last_options': None,\n",
    "    'df_candidates': None,\n",
    "    'df_suggestions': None,\n",
    "    'df_mapeo': None,\n",
    "    'df_unicos_revisado': None,\n",
    "    'review_idx': 0,\n",
    "    'review_log': []\n",
    "}\n",
    "\n",
    "# Widgets de control\n",
    "manual_path_text = Text(\n",
    "    value='',\n",
    "    description='Ruta manual:',\n",
    "    placeholder='input/archivo.xlsx',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "sheet_pattern_text = Text(\n",
    "    value=r'^\\d{3}$',\n",
    "    description='Patr√≥n hojas:',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "max_sheet_input = IntText(\n",
    "    value=0,\n",
    "    description='Max hojas (0=todas):',\n",
    "    layout=widgets.Layout(width='220px')\n",
    ")\n",
    "\n",
    "include_aux_checkbox = Checkbox(value=True, description='Incluir Aux.*')\n",
    "include_otro_checkbox = Checkbox(value=False, description='Incluir OTRO')\n",
    "debug_checkbox = Checkbox(value=False, description='Debug: HOJA/CODIGO')\n",
    "\n",
    "csv_path_text = Text(\n",
    "    value=str(OUTPUT_FOLDER / 'unicos.csv'),\n",
    "    description='CSV path:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "xlsx_path_text = Text(\n",
    "    value=str(OUTPUT_FOLDER / 'unicos.xlsx'),\n",
    "    description='XLSX path:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "execute_button = Button(\n",
    "    description='‚ñ∂ Ejecutar ETL',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "export_button = Button(\n",
    "    description='üíæ Exportar',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "output_status = Output()\n",
    "output_tables = Output()\n",
    "output_explore = Output()\n",
    "\n",
    "# Exploraci√≥n\n",
    "recurso_filter_dropdown = Dropdown(\n",
    "    options=['(Todos)'],\n",
    "    value='(Todos)',\n",
    "    description='RECURSO:',\n",
    "    layout=widgets.Layout(width='220px')\n",
    ")\n",
    "\n",
    "descripcion_search_text = Text(\n",
    "    value='',\n",
    "    description='Buscar:',\n",
    "    placeholder='Texto en DESCRIPCION',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "apply_filters_button = Button(\n",
    "    description='Aplicar filtros',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='160px')\n",
    ")\n",
    "\n",
    "# Similitud\n",
    "sim_review_button = Button(\n",
    "    description='üîç Iniciar revisi√≥n similitud',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='220px')\n",
    ")\n",
    "\n",
    "sim_accept_button = Button(\n",
    "    description='‚úÖ Aceptar sugerencia',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='180px')\n",
    ")\n",
    "\n",
    "sim_keep_button = Button(\n",
    "    description='‚ûñ Mantener original',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='170px')\n",
    ")\n",
    "\n",
    "sim_skip_button = Button(\n",
    "    description='‚è≠ Saltar',\n",
    "    button_style='',\n",
    "    layout=widgets.Layout(width='120px')\n",
    ")\n",
    "\n",
    "sim_custom_text = Text(\n",
    "    value='',\n",
    "    description='Reemplazo manual:',\n",
    "    placeholder='Escribe y presiona Enter',\n",
    "    layout=widgets.Layout(width='420px')\n",
    ")\n",
    "\n",
    "sim_apply_custom_button = Button(\n",
    "    description='‚úèÔ∏è Aplicar texto',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "sim_export_button = Button(\n",
    "    description='üíæ Exportar similitud',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "sim_status_html = HTML()\n",
    "sim_output = Output()\n",
    "\n",
    "\n",
    "def _resolve_source_path() -> Optional[str]:\n",
    "    manual = manual_path_text.value.strip()\n",
    "    if manual:\n",
    "        return manual\n",
    "    return current_file_path.get('path')\n",
    "\n",
    "\n",
    "def _update_resource_filter(df: pd.DataFrame):\n",
    "    if df is None or df.empty:\n",
    "        recurso_filter_dropdown.options = ['(Todos)']\n",
    "        recurso_filter_dropdown.value = '(Todos)'\n",
    "        return\n",
    "    valores = sorted([v for v in df['RECURSO'].dropna().unique().tolist()])\n",
    "    recurso_filter_dropdown.options = ['(Todos)'] + valores\n",
    "    recurso_filter_dropdown.value = '(Todos)'\n",
    "\n",
    "\n",
    "def _reset_similarity_state():\n",
    "    state['df_candidates'] = None\n",
    "    state['df_suggestions'] = None\n",
    "    state['df_mapeo'] = None\n",
    "    state['df_unicos_revisado'] = None\n",
    "    state['review_idx'] = 0\n",
    "    state['review_log'] = []\n",
    "    sim_status_html.value = ''\n",
    "    sim_output.clear_output()\n",
    "\n",
    "\n",
    "def on_execute_click(b):\n",
    "    if execute_button.disabled:\n",
    "        return\n",
    "    output_status.clear_output()\n",
    "    output_tables.clear_output()\n",
    "    output_explore.clear_output()\n",
    "\n",
    "    source = _resolve_source_path()\n",
    "    if not source:\n",
    "        with output_status:\n",
    "            print(\"‚ùå Por favor sube un archivo o ingresa una ruta manual\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(source):\n",
    "        with output_status:\n",
    "            print(f\"‚ùå Archivo no encontrado: {source}\")\n",
    "        return\n",
    "\n",
    "    options = {\n",
    "        'sheet_pattern': sheet_pattern_text.value,\n",
    "        'max_sheet': max_sheet_input.value if max_sheet_input.value > 0 else None,\n",
    "        'include_aux': include_aux_checkbox.value,\n",
    "        'include_otro': include_otro_checkbox.value,\n",
    "        'debug': debug_checkbox.value,\n",
    "        'progress_every': 20\n",
    "    }\n",
    "\n",
    "    # Bloquear bot√≥n mientras corre\n",
    "    execute_button.disabled = True\n",
    "    export_button.disabled = True\n",
    "    execute_button.description = '‚è≥ Ejecutando...'\n",
    "    execute_button.button_style = 'warning'\n",
    "\n",
    "    try:\n",
    "        with output_status:\n",
    "            print(\"üöÄ Ejecutando ETL...\")\n",
    "            print(f\"Archivo: {source}\")\n",
    "            print(f\"Opciones: {options}\")\n",
    "            results = run_etl(source, options)\n",
    "    except Exception as e:\n",
    "        with output_status:\n",
    "            print(f\"‚ùå Error en ETL: {e}\")\n",
    "        return\n",
    "    finally:\n",
    "        execute_button.disabled = False\n",
    "        export_button.disabled = False\n",
    "        execute_button.description = '‚ñ∂ Ejecutar ETL'\n",
    "        execute_button.button_style = 'success'\n",
    "\n",
    "    state['df_unicos'] = results['df_unicos']\n",
    "    state['df_sheet_summary'] = results['df_sheet_summary']\n",
    "    state['df_debug'] = results['df_debug']\n",
    "    state['df_debug_agg'] = results['df_debug_agg']\n",
    "    state['stats'] = results['stats']\n",
    "    state['last_options'] = options\n",
    "\n",
    "    _update_resource_filter(state['df_unicos'])\n",
    "    _reset_similarity_state()\n",
    "\n",
    "    # Resumen en output_status\n",
    "    stats = results['stats']\n",
    "    with output_status:\n",
    "        print(\"\")\n",
    "        print(\"‚úÖ ETL completado\")\n",
    "        print(f\"Hojas procesadas OK: {stats['sheets_ok']}/{stats['total_sheets']}\")\n",
    "        print(f\"Hojas fallidas: {stats['sheets_failed']}\")\n",
    "        print(f\"Filas le√≠das: {stats['total_rows_read']}\")\n",
    "        print(f\"Filas filtradas: {stats['total_rows_filtered']}\")\n",
    "        print(f\"Filas extra√≠das: {stats['total_rows_extracted']}\")\n",
    "        print(f\"Duplicados removidos: {stats['duplicates_removed']}\")\n",
    "        print(f\"Filas finales (UNICOS): {stats['final_rows']}\")\n",
    "        print(f\"Tiempo: {stats['time_seconds']:.2f}s\")\n",
    "\n",
    "        failed = stats.get('failed_sheet_details', [])\n",
    "        if failed:\n",
    "            print(\"\")\n",
    "            print(\"‚ùó Top 5 errores:\")\n",
    "            for item in failed[:5]:\n",
    "                print(f\"- {item.get('HOJA')}: {item.get('ERROR')}\")\n",
    "\n",
    "    # Tablas en output_tables\n",
    "    with output_tables:\n",
    "        print(\"\")\n",
    "        print(\"üìÑ Resumen por hoja:\")\n",
    "        display(state['df_sheet_summary'])\n",
    "\n",
    "        if state['df_unicos'] is None or state['df_unicos'].empty:\n",
    "            print(\"\")\n",
    "            print(\"‚ö†Ô∏è df_unicos est√° vac√≠o\")\n",
    "        else:\n",
    "            print(\"\")\n",
    "            print(\"üìã Preview df_unicos (primeras 20 filas):\")\n",
    "            display(state['df_unicos'].head(20))\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"üìä Conteo por RECURSO:\")\n",
    "            display(state['df_unicos']['RECURSO'].value_counts().reset_index().rename(columns={'index': 'RECURSO', 'RECURSO': 'COUNT'}))\n",
    "\n",
    "        if state['df_debug_agg'] is not None:\n",
    "            print(\"\")\n",
    "            print(\"üß™ Debug agregado (primeras 20 filas):\")\n",
    "            display(state['df_debug_agg'].head(20))\n",
    "\n",
    "\n",
    "def on_export_click(b):\n",
    "    output_status.clear_output()\n",
    "    df = state.get('df_unicos')\n",
    "    ok, msg = export_results(df, csv_path_text.value, xlsx_path_text.value)\n",
    "    with output_status:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def on_apply_filters(b):\n",
    "    output_explore.clear_output()\n",
    "    df = state.get('df_unicos')\n",
    "    if df is None or df.empty:\n",
    "        with output_explore:\n",
    "            print(\"‚ö†Ô∏è No hay datos para filtrar. Ejecuta el ETL primero.\")\n",
    "        return\n",
    "\n",
    "    filtered = df.copy()\n",
    "    recurso = recurso_filter_dropdown.value\n",
    "    if recurso and recurso != '(Todos)':\n",
    "        filtered = filtered[filtered['RECURSO'] == recurso]\n",
    "\n",
    "    q = descripcion_search_text.value.strip()\n",
    "    if q:\n",
    "        filtered = filtered[filtered['DESCRIPCION'].astype(str).str.contains(q, case=False, na=False, regex=False)]\n",
    "\n",
    "    with output_explore:\n",
    "        print(f\"Filas filtradas: {len(filtered)}\")\n",
    "        display(filtered.head(50))\n",
    "        print(\"\")\n",
    "        print(\"Conteo por RECURSO:\")\n",
    "        display(filtered['RECURSO'].value_counts().reset_index().rename(columns={'index': 'RECURSO', 'RECURSO': 'COUNT'}))\n",
    "\n",
    "\n",
    "def _current_suggestion():\n",
    "    df = state.get('df_suggestions')\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    idx = state.get('review_idx', 0)\n",
    "    if idx >= len(df):\n",
    "        return None\n",
    "    return df.iloc[idx]\n",
    "\n",
    "\n",
    "def _render_current():\n",
    "    row = _current_suggestion()\n",
    "    if row is None:\n",
    "        # finalizar\n",
    "        if state['review_log']:\n",
    "            df_mapeo = pd.DataFrame(state['review_log'])\n",
    "            state['df_mapeo'] = df_mapeo\n",
    "            mapping = {r['ORIGINAL']: r['ELEGIDO'] for r in state['review_log'] if r.get('ACCION') != 'SALTAR' and r.get('ELEGIDO')}\n",
    "            df_rev = state['df_unicos'].copy()\n",
    "            df_rev['DESCRIPCION'] = df_rev['DESCRIPCION'].apply(lambda x: mapping.get(x, x))\n",
    "            state['df_unicos_revisado'] = df_rev\n",
    "\n",
    "            sim_status_html.value = \"<b>‚úÖ Revisi√≥n finalizada.</b> Puedes exportar el XLSX de similitud.\"\n",
    "            with sim_output:\n",
    "                print(\"\n",
    "Resumen de cambios:\")\n",
    "                display(df_mapeo)\n",
    "        else:\n",
    "            sim_status_html.value = \"<b>‚úÖ Revisi√≥n finalizada.</b> Sin cambios registrados.\"\n",
    "        return\n",
    "\n",
    "    total = len(state['df_suggestions'])\n",
    "    idx = state.get('review_idx', 0) + 1\n",
    "    sim_status_html.value = (\n",
    "        f\"<b>Revisi√≥n {idx}/{total}</b><br>\"\n",
    "        f\"RECURSO: <b>{row['RECURSO']}</b><br>\"\n",
    "        f\"SIMILARIDAD: <b>{row['SIMILARIDAD']}%</b><br>\"\n",
    "        f\"ORIGINAL: <code>{row['ORIGINAL']}</code><br>\"\n",
    "        f\"SIMILAR: <code>{row['SIMILAR']}</code><br>\"\n",
    "        f\"PROPUESTA: <b>{row['PROPUESTA']}</b>\"\n",
    "    )\n",
    "    sim_custom_text.value = ''\n",
    "\n",
    "\n",
    "def on_start_similarity(b):\n",
    "    sim_output.clear_output()\n",
    "    df = state.get('df_unicos')\n",
    "    if df is None or df.empty:\n",
    "        sim_status_html.value = \"<b>‚ö†Ô∏è Ejecuta el ETL antes de revisar similitud.</b>\"\n",
    "        return\n",
    "\n",
    "    sim_status_html.value = \"<b>‚è≥ Generando candidatos de similitud...</b>\"\n",
    "    df_candidates = find_similar_candidates(df, threshold=SIM_THRESHOLD, scope=SIM_SCOPE, progress_every=SIM_PROGRESS_EVERY)\n",
    "    state['df_candidates'] = df_candidates\n",
    "\n",
    "    if df_candidates.empty:\n",
    "        sim_status_html.value = \"<b>‚úÖ No se encontraron candidatos con el umbral actual.</b>\"\n",
    "        return\n",
    "\n",
    "    df_suggestions = build_canonical_map(df, df_candidates)\n",
    "    state['df_suggestions'] = df_suggestions\n",
    "    state['review_idx'] = 0\n",
    "    state['review_log'] = []\n",
    "\n",
    "    with sim_output:\n",
    "        print(f\"Candidatos: {len(df_candidates)}\")\n",
    "\n",
    "    _render_current()\n",
    "\n",
    "\n",
    "def _record_action(action: str, elegido: str = ''):\n",
    "    row = _current_suggestion()\n",
    "    if row is None:\n",
    "        return\n",
    "\n",
    "    state['review_log'].append({\n",
    "        'ORIGINAL': row['ORIGINAL'],\n",
    "        'SIMILAR': row['SIMILAR'],\n",
    "        'SUGERIDO': row['PROPUESTA'],\n",
    "        'ELEGIDO': elegido,\n",
    "        'SIMILARIDAD': row['SIMILARIDAD'],\n",
    "        'RECURSO': row['RECURSO'],\n",
    "        'ACCION': action\n",
    "    })\n",
    "\n",
    "    with sim_output:\n",
    "        if action != 'SALTAR':\n",
    "            print(f\"{action}: '{row['ORIGINAL']}' -> '{elegido}' (sim {row['SIMILARIDAD']}%)\")\n",
    "        else:\n",
    "            print(f\"{action}: '{row['ORIGINAL']}' (sim {row['SIMILARIDAD']}%)\")\n",
    "\n",
    "    state['review_idx'] += 1\n",
    "    _render_current()\n",
    "\n",
    "\n",
    "def on_accept_click(b):\n",
    "    row = _current_suggestion()\n",
    "    if row is None:\n",
    "        return\n",
    "    _record_action('ACEPTAR', row['PROPUESTA'])\n",
    "\n",
    "\n",
    "def on_keep_click(b):\n",
    "    row = _current_suggestion()\n",
    "    if row is None:\n",
    "        return\n",
    "    _record_action('MANTENER', row['ORIGINAL'])\n",
    "\n",
    "\n",
    "def on_skip_click(b):\n",
    "    _record_action('SALTAR', '')\n",
    "\n",
    "\n",
    "def on_apply_custom(b):\n",
    "    custom = sim_custom_text.value.strip()\n",
    "    if not custom:\n",
    "        return\n",
    "    _record_action('CUSTOM', custom)\n",
    "\n",
    "\n",
    "def on_export_similarity(b):\n",
    "    ok, msg = export_similarity_results(state.get('df_unicos_revisado'), state.get('df_mapeo'), SIM_EXPORT_PATH)\n",
    "    with sim_output:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def _bind_enter(text_widget, handler):\n",
    "    try:\n",
    "        text_widget.on_submit(lambda x: handler(None))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "execute_button.on_click(on_execute_click)\n",
    "export_button.on_click(on_export_click)\n",
    "apply_filters_button.on_click(on_apply_filters)\n",
    "\n",
    "sim_review_button.on_click(on_start_similarity)\n",
    "sim_accept_button.on_click(on_accept_click)\n",
    "sim_keep_button.on_click(on_keep_click)\n",
    "sim_skip_button.on_click(on_skip_click)\n",
    "sim_apply_custom_button.on_click(on_apply_custom)\n",
    "sim_export_button.on_click(on_export_similarity)\n",
    "_bind_enter(sim_custom_text, on_apply_custom)\n",
    "\n",
    "panel = VBox([\n",
    "    HTML('<h2>‚öôÔ∏è Panel de Control ETL</h2>'),\n",
    "    input_ui,\n",
    "    HTML('<h3>üîß Par√°metros</h3>'),\n",
    "    manual_path_text,\n",
    "    HBox([sheet_pattern_text, max_sheet_input]),\n",
    "    HBox([include_aux_checkbox, include_otro_checkbox, debug_checkbox]),\n",
    "    HTML('<h3>üíæ Exportaci√≥n</h3>'),\n",
    "    csv_path_text,\n",
    "    xlsx_path_text,\n",
    "    HBox([execute_button, export_button]),\n",
    "    output_status,\n",
    "    HTML('<hr/>'),\n",
    "    HTML('<h3>üìä Resultados</h3>'),\n",
    "    output_tables,\n",
    "    HTML('<h3>üîé Exploraci√≥n</h3>'),\n",
    "    HBox([recurso_filter_dropdown, descripcion_search_text, apply_filters_button]),\n",
    "    output_explore,\n",
    "    HTML('<hr/>'),\n",
    "    HTML('<h3>üß† Similitud heur√≠stica</h3>'),\n",
    "    sim_review_button,\n",
    "    sim_status_html,\n",
    "    HBox([sim_accept_button, sim_keep_button, sim_skip_button]),\n",
    "    HBox([sim_custom_text, sim_apply_custom_button]),\n",
    "    sim_export_button,\n",
    "    sim_output\n",
    "])\n",
    "\n",
    "display(panel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ How to use (r√°pido)\n",
    "\n",
    "1. Ejecuta **Run All** en el notebook.\n",
    "2. Sube tu archivo Excel con el widget de carga o ingresa una ruta manual.\n",
    "3. Ajusta par√°metros (patr√≥n de hojas, m√°ximo, incluir Aux, incluir OTRO, Debug).\n",
    "4. Presiona **‚ñ∂ Ejecutar ETL**.\n",
    "5. Explora resultados con los filtros (RECURSO y b√∫squeda por DESCRIPCION).\n",
    "6. Exporta con **üíæ Exportar** (CSV/XLSX).\n",
    "\n",
    "**Paso adicional (Similitud heur√≠stica):**\n",
    "1. Presiona **üîç Iniciar revisi√≥n similitud**.\n",
    "2. Revisa uno‚Äëpor‚Äëuno y elige: Aceptar, Mantener, Saltar o escribir un reemplazo.\n",
    "3. Al finalizar, exporta con **üíæ Exportar similitud** (XLSX con hojas UNICOS + MAPEO).\n",
    "\n",
    "**Outputs generados:**\n",
    "- `df_unicos`: consolidado final (contrato sin columnas extra si `debug=False`).\n",
    "- `df_sheet_summary`: resumen por hoja.\n",
    "- `df_debug` y `df_debug_agg`: solo si `debug=True`.\n",
    "- `df_mapeo` y `df_unicos_revisado`: solo si completas revisi√≥n de similitud.\n",
    "\n",
    "**Debug ON**: activa trazabilidad (HOJA, CODIGO, ROW_INDEX) y un agregado con HOJAS/CODIGOS por recurso.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}